{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dda1522",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries  Machine Learning for Predictive Analytics \n",
    "#a classifier that uses data to predict if a patient has diabetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea85ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pandas as pd\n",
    "from tabulate import tabulate\n",
    "import numpy as np\n",
    "import pandas as df \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OrdinalEncoder \n",
    "from mlxtend.frequent_patterns import apriori,association_rules\n",
    "import pandas as pd\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "import time\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier # Import Decision Tree Classifier\n",
    "from sklearn.model_selection import train_test_split # Import train_test_split function\n",
    "from sklearn import metrics #Import scikit-learn metrics module for accuracy calculation\n",
    "#%pip install six\n",
    "import six\n",
    "import sys\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "# plotly library\n",
    "import plotly as py\n",
    "from plotly.offline import init_notebook_mode, iplot\n",
    "init_notebook_mode(connected=True)\n",
    "import plotly.graph_objs as go\n",
    "# word cloud library\n",
    "from wordcloud import WordCloud\n",
    "# matplotlib library\n",
    "import matplotlib.pyplot as plt\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "sys.modules['sklearn.externals.six'] = six\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import plotly as py\n",
    "import plotly.graph_objs as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd906d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the data\n",
    "\n",
    "df = pd.read_csv(\"diabetes.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371c4ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "styled_df = df.head(5).style\n",
    "\n",
    "# Set background color, text color, and border for the entire DataFrame\n",
    "styled_df.set_properties(**{\"background-color\": \"#254E58\", \"color\": \"#e9c46a\", \"border\": \"1.5px solid black\"})\n",
    "\n",
    "# Modify the color and background color of the table headers (th)\n",
    "styled_df.set_table_styles([\n",
    "    {\"selector\": \"th\", \"props\": [(\"color\", 'white'), (\"background-color\", \"#333333\")]}\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1497a6b0",
   "metadata": {},
   "source": [
    "3.2.2 | Number of rows, number of columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4d649e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows , col =  df.shape\n",
    "print(f\"Number of Rows : {rows} \\nNumber of Columns : {col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32ab8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749d37df",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744fa413",
   "metadata": {},
   "source": [
    "3.2.5 | Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341bf59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "styled_df = df.describe().style \\\n",
    "    .set_table_styles([\n",
    "        {'selector': 'th', 'props': [('background-color', '#254E58'), ('color', 'white'), ('font-weight', 'bold'), ('text-align', 'left'), ('padding', '8px')]},\n",
    "        {'selector': 'td', 'props': [('padding', '8px')]}\n",
    "    ]) \\\n",
    "    .set_properties(**{'font-size': '14px', 'background-color': '#F5F5F5', 'border-collapse': 'collapse', 'margin': '10px'})\n",
    "\n",
    "# Display the styled DataFrame\n",
    "styled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35fd59c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import missingno as msno\n",
    "msno.bar(df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f949d2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.hist(figsize = (10,10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937f6a3f",
   "metadata": {},
   "source": [
    "3.3.2 | Boxplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d677c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rows, num_cols = 3, 3\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(num_rows, num_cols, figsize=(15, 10))\n",
    "\n",
    "# Flatten the axes for easier iteration\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Loop through numeric columns and create boxplots\n",
    "for i, column in enumerate(df.columns):\n",
    "    sns.boxplot(data=df, x=column, ax=axes[i])\n",
    "    axes[i].set_title(f'Boxplot for {column}')\n",
    "\n",
    "# Remove any remaining empty subplots\n",
    "for j in range(len(df.columns), len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3e32db",
   "metadata": {},
   "source": [
    "3.3.3 | Pairplot of attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261c5080",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(data = df, hue = 'Outcome' )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58794bf5",
   "metadata": {},
   "source": [
    "3.3.4 | Age and Outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad12fd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc={\"axes.facecolor\":\"#EAE7F9\",\"figure.facecolor\":\"#EAE7F9\"})\n",
    "p=sns.catplot(x=\"Outcome\",y=\"Age\", data=df, kind='box')\n",
    "plt.title(\"Age and Outcome Correlation\", size=20, y=1.0);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1750d744",
   "metadata": {},
   "source": [
    "3.3.5 | Glucose and Outcome Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d0603c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc={\"axes.facecolor\":\"#EAE7F9\",\"figure.facecolor\":\"#EAE7F9\"})\n",
    "p=sns.catplot(x=\"Outcome\",y=\"Glucose\", data=df, kind='box')\n",
    "plt.title(\"Glucose and Outcome Correlation\", size=20, y=1.0);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1572c9",
   "metadata": {},
   "source": [
    "3.3.6 | Correlation between attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a105a775",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 17))\n",
    "matrix = np.triu(df.corr())\n",
    "sns.heatmap(df.corr(), annot=True, linewidth=.8, mask=matrix, cmap=\"rocket\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa04ae28",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,9))\n",
    "sns.heatmap(df.corr(), annot=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad0d0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "hig_corr = df.corr()\n",
    "hig_corr_features = hig_corr.index[abs(hig_corr[\"Outcome\"]) >= 0.2]\n",
    "hig_corr_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50564c4",
   "metadata": {},
   "source": [
    "3.3.7 | Standard Deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9600c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standard Deviation\n",
    "df.var()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff97572c",
   "metadata": {},
   "source": [
    " 4 | Data Preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe6387f",
   "metadata": {},
   "source": [
    "Handling Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc81f299",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_columns = ['Insulin', 'DiabetesPedigreeFunction',]\n",
    "\n",
    "for column_name in numeric_columns:\n",
    "    Q1 = np.percentile(df[column_name], 25, interpolation='midpoint')\n",
    "    Q3 = np.percentile(df[column_name], 75, interpolation='midpoint')\n",
    "\n",
    "    IQR = Q3 - Q1\n",
    "    low_lim = Q1 - 1.5 * IQR\n",
    "    up_lim = Q3 + 1.5 * IQR\n",
    "\n",
    "    # Find outliers in the specified column\n",
    "    outliers = df[(df[column_name] < low_lim) | (df[column_name] > up_lim)][column_name]\n",
    "\n",
    "    # Replace outliers with the respective lower or upper limit\n",
    "    df[column_name] = np.where(df[column_name] < low_lim, low_lim, df[column_name])\n",
    "    df[column_name] = np.where(df[column_name] > up_lim, up_lim, df[column_name])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91f79ec",
   "metadata": {},
   "source": [
    "Get `input` and `target` columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08886fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('Outcome', axis = 1)\n",
    "y = df['Outcome']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6513470b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data for the bar plot (number of instances for each column)\n",
    "data = {\n",
    "    'Column': ['Index', 'Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age', 'Outcome'],\n",
    "    'Count': [6136, 6136, 6136, 6136, 6136, 6136, 6136, 6136, 6136, 6136]\n",
    "}\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Plot the number of instances for each column\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(df['Column'], df['Count'], color='b', alpha=0.7)\n",
    "plt.xlabel('Column')\n",
    "plt.ylabel('Number of Instances')\n",
    "plt.title('Number of Instances for Each Column')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6546aef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data for the first three data entries\n",
    "data = pd.DataFrame({\n",
    "    'Pregnancies': [1, 8, 1],\n",
    "    'Glucose': [85, 183, 89],\n",
    "    'BloodPressure': [66, 64, 66],\n",
    "    'SkinThickness': [29, 0, 23],\n",
    "    'Insulin': [0, 0, 94],\n",
    "    'BMI': [26.6, 23.3, 28.1],\n",
    "    'DiabetesPedigreeFunction': [0.351, 0.672, 0.167],\n",
    "    'Age': [31, 32, 21],\n",
    "    'Outcome': [0, 1, 0]\n",
    "})\n",
    "\n",
    "# Print the first three data entries as a table using tabulate\n",
    "print(tabulate(data.head(3), headers='keys', tablefmt='psql'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba94494a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to print the last ten data entries using head().\n",
    "data.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbd9903",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to get the features and its data type using info()\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1577bce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming 'data' is your dictionary containing the dataset\n",
    "# Convert the dictionary to a pandas DataFrame\n",
    "df = pd.DataFrame.from_dict(data)\n",
    "\n",
    "# Obtain the data types of each column\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78498dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Sample data \n",
    "data = {\n",
    "    'Pregnancies': [6, 1, 8, 1, 0],\n",
    "    'Glucose': [148, 85, 183, 89, 137],\n",
    "    'BloodPressure': [72, 66, 64, 66, 40],\n",
    "    'SkinThickness': [35, 29, 0, 23, 35],\n",
    "    'Insulin': [0, 0, 0, 94, 168],\n",
    "    'BMI': [33.6, 26.6, 23.3, 28.1, 43.1],\n",
    "    'DiabetesPedigreeFunction': [0.627, 0.351, 0.672, 0.167, 2.288],\n",
    "    'Age': [50, 31, 32, 21, 33],\n",
    "    'Outcome': [1, 0, 1, 0, 1]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Bar Plot - Count of Outcome Categories\n",
    "outcome_counts = df['Outcome'].value_counts()\n",
    "plt.bar(outcome_counts.index, outcome_counts.values)\n",
    "plt.xlabel('Outcome')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Count of Outcome Categories')\n",
    "plt.xticks(outcome_counts.index, ['Not Diabetic', 'Diabetic'])\n",
    "plt.show()\n",
    "\n",
    "# Histogram - Age Distribution\n",
    "plt.hist(df['Age'], bins=20, edgecolor='k')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Age Distribution')\n",
    "plt.show()\n",
    "\n",
    "# Scatter Plot - BMI vs. Glucose with Outcome Color-Coding\n",
    "plt.scatter(df['BMI'], df['Glucose'], c=df['Outcome'], cmap='coolwarm', alpha=0.8)\n",
    "plt.xlabel('BMI')\n",
    "plt.ylabel('Glucose')\n",
    "plt.title('BMI vs. Glucose with Outcome Color-Coding')\n",
    "plt.colorbar(label='Outcome')\n",
    "plt.show()\n",
    "\n",
    "# Box Plot - Glucose Levels by Outcome\n",
    "plt.boxplot([df[df['Outcome'] == 0]['Glucose'], df[df['Outcome'] == 1]['Glucose']], labels=['Not Diabetic', 'Diabetic'])\n",
    "plt.ylabel('Glucose')\n",
    "plt.title('Glucose Levels by Outcome')\n",
    "plt.show()\n",
    "\n",
    "# Correlation Heatmap\n",
    "import seaborn as sns\n",
    "\n",
    "correlation_matrix = df.corr()\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412c76eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming 'data' is your dictionary\n",
    "data = {'column1': [1, 2, 3], 'column2': ['A', 'B', 'C']}\n",
    "\n",
    "# Convert the dictionary to a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Now you can use the 'dtypes' attribute to get the data types of columns\n",
    "data_types_counts = df.dtypes.value_counts()\n",
    "print(data_types_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51e72ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'data' is your dictionary\n",
    "data = {'column1': [1, 2, 3], 'column2': ['A', 'B', 'C']}\n",
    "\n",
    "# Convert the dictionary to a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Now you can use the methods and attributes on the DataFrame\n",
    "# To obtain the summary statistics\n",
    "print(df.describe())\n",
    "\n",
    "# To obtain the count of each column\n",
    "print(df.count())\n",
    "\n",
    "# To obtain the values of each column\n",
    "print(df.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc400602",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'data' is your dictionary\n",
    "data = {'column1': [1, 2, 3, None], 'column2': ['A', 'B', None, 'C']}\n",
    "\n",
    "# Convert the dictionary to a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Now you can use the method on the DataFrame to determine the presence of missing values\n",
    "print(df.isna())\n",
    "\n",
    "# Get the count of missing values in each column\n",
    "print(df.isna().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc812ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'data' is your dictionary\n",
    "data = {'column1': [1, 2, 3, None], 'column2': ['A', 'B', None, 'C']}\n",
    "\n",
    "# Convert the dictionary to a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Now you can use the method on the DataFrame to drop rows with missing values\n",
    "df_dropped = df.dropna()\n",
    "\n",
    "# The variable 'df_dropped' now contains the DataFrame with rows containing missing values removed.\n",
    "print(df_dropped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882941a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'key1': [1, 2, 3, None, 5],\n",
    "    'key2': [None, 'a', 'b', 'c', 'd']\n",
    "}\n",
    "\n",
    "# Determine the presence of missing values.\n",
    "missing_values = {key: [value is None for value in values] for key, values in data.items()}\n",
    "print(missing_values)\n",
    "\n",
    "# Get the count of missing values.\n",
    "missing_counts = {key: sum(value is None for value in values) for key, values in data.items()}\n",
    "print(missing_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08307041",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data for learning rate and accuracy scores\n",
    "learning_rates = [0.05, 0.075, 0.1, 0.25, 0.5, 0.75, 1]\n",
    "training_scores = [0.767, 0.779, 0.795, 0.819, 0.834, 0.862, 0.857]\n",
    "validation_scores = [0.739, 0.752, 0.771, 0.784, 0.784, 0.739, 0.771]\n",
    "\n",
    "# Plotting the learning rate against the training and validation accuracy scores\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(learning_rates, training_scores, marker='o', label='Training Accuracy')\n",
    "plt.plot(learning_rates, validation_scores, marker='o', label='Validation Accuracy')\n",
    "plt.xlabel('Learning Rate')\n",
    "plt.ylabel('Accuracy Score')\n",
    "plt.title('Learning Rate vs. Accuracy Score')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e16b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data for accuracy scores\n",
    "accuracy_scores = [1.000, 0.752]\n",
    "labels = ['Training Accuracy', 'Validation Accuracy']\n",
    "\n",
    "# Plotting the accuracy scores\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.bar(labels, accuracy_scores, color=['blue', 'orange'])\n",
    "plt.ylabel('Accuracy Score')\n",
    "plt.title('Training vs. Validation Accuracy')\n",
    "plt.ylim(0, 1.1)\n",
    "plt.grid(axis='y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95348004",
   "metadata": {},
   "source": [
    "Splitting data for the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065d365e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729d80d6",
   "metadata": {},
   "source": [
    "5 | Machine Learning models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2febee8",
   "metadata": {},
   "source": [
    "5.1 | Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc09288c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "log_reg = LogisticRegression(C=1, penalty='l2', solver='liblinear', max_iter=200)\n",
    "log_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3c6c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def predict_and_plot(model, inputs, targets, name=''):\n",
    "    preds = model.predict(inputs)\n",
    "    accuracy = accuracy_score(targets, preds)\n",
    "    print(\"Accuracy: {:.2f}%\".format(accuracy * 100))\n",
    "    \n",
    "    cf = confusion_matrix(targets, preds, normalize='true')\n",
    "    plt.figure()\n",
    "    sns.heatmap(cf, annot=True)\n",
    "    plt.xlabel('Prediction')\n",
    "    plt.ylabel('Target')\n",
    "    plt.title('{} Confusion Matrix'.format(name))\n",
    "    \n",
    "    return preds\n",
    "\n",
    "# Predict and plot on the training data\n",
    "train_preds = predict_and_plot(log_reg, X_train, y_train, 'Train')\n",
    "\n",
    "# Predict and plot on the validation data\n",
    "val_preds = predict_and_plot(log_reg, X_test, y_test, 'Validation')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1e76dcce",
   "metadata": {},
   "source": [
    "Evaluate: Logistic Regression Model\n",
    "Training Accuracy - 77.54%\n",
    "Validation Accuracy - 77.68%\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2390e7d4",
   "metadata": {},
   "source": [
    "5.2 | Random Forest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f492f8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model_2 = RandomForestClassifier(n_jobs =-1, random_state = 42)\n",
    "model_2.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b553ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2.score(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8050978",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_and_plot(model, inputs,targets, name = ''):\n",
    "    preds = model.predict(inputs)\n",
    "    accuracy = accuracy_score(targets, preds)\n",
    "    print(\"Accuracy: {:.2f}%\".format(accuracy*100))\n",
    "    \n",
    "    cf = confusion_matrix(targets, preds, normalize = 'true')\n",
    "    plt.figure()\n",
    "    sns.heatmap(cf, annot = True)\n",
    "    plt.xlabel('Prediction')\n",
    "    plt.ylabel('Target')\n",
    "    plt.title('{} Confusion Matrix'. format(name))\n",
    "    \n",
    "    return preds\n",
    "\n",
    "# Predict and plot on the training data\n",
    "train_preds = predict_and_plot(model_2, X_train, y_train, 'Train')\n",
    "\n",
    "# Predict and plot on the validation data\n",
    "val_preds = predict_and_plot(model_2, X_test, y_test, 'Validation')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d6fd173f",
   "metadata": {},
   "source": [
    "Evaluate: Random Forest Model: Before Tunning\n",
    "Training Accuracy - 96.00%\n",
    "Validation Accuracy - 78.08%\n",
    "This model seems to be overfitting as training accuracy is very high and the validation accuracy is not so."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674d862f",
   "metadata": {},
   "source": [
    "Hyperparameter Tuning of Random Forest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f90da3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [10, 20, 30],  # Adjust the number of trees in the forest\n",
    "    'max_depth': [10, 20, 30],  # Adjust the maximum depth of each tree\n",
    "    'min_samples_split': [2, 5, 10, 15, 20],  # Adjust the minimum samples required to split a node\n",
    "    'min_samples_leaf': [1, 2, 4, 6, 8]  # Adjust the minimum samples required in a leaf node\n",
    "}\n",
    "\n",
    "model = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, n_jobs=-1, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on the training and validation data\n",
    "train_accuracy = best_model.score(X_train, y_train)\n",
    "val_accuracy = best_model.score(X_test, y_test)\n",
    "\n",
    "# Print the results\n",
    "print(\"Training Accuracy:\", train_accuracy)\n",
    "print(\"Validation Accuracy:\", val_accuracy)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ffd81e84",
   "metadata": {},
   "source": [
    "Evaluate: Random Forest Model After Hyper Parametic Tunning\n",
    "Training Accuracy - 89.2%\n",
    "Validation Accuracy - 87.6%\n",
    "It has reduced overfitting compared to the initial model.And Improve the accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca38f89",
   "metadata": {},
   "source": [
    "5.3 |Decision Tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e818f440",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Create the Decision Tree model\n",
    "decision_tree_model = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Fit the model to the training data\n",
    "decision_tree_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on the training and validation data\n",
    "train_accuracy = decision_tree_model.score(X_train, y_train)\n",
    "val_accuracy = decision_tree_model.score(X_test, y_test)\n",
    "\n",
    "# Print the results\n",
    "print(\"Training Accuracy:\", train_accuracy)\n",
    "print(\"Validation Accuracy:\", val_accuracy)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e22e7615",
   "metadata": {},
   "source": [
    "Evaluate: Decision Tree Model: Before Tunning\n",
    "Training Accuracy - 100%\n",
    "Validation Accuracy - 75.0%\n",
    "Tree model is overfitting the training data, as it's achieving perfect accuracy on the training data but lower accuracy on the validation data.."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c982016",
   "metadata": {},
   "source": [
    "Hyperparameter Tuning Of Decision Tree\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df24d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'max_depth': [None, 5, 10, 15, 20],\n",
    "    'min_samples_split': [2, 5, 10, 15, 20, 25],\n",
    "    'min_samples_leaf': [1, 3, 5, 7],\n",
    "    'criterion': ['gini', 'entropy']  # Add criterion hyperparameter\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# Create the Decision Tree model\n",
    "decision_tree_model = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Perform GridSearchCV for hyperparameter tuning\n",
    "grid_search = GridSearchCV(decision_tree_model, param_grid, cv=5, n_jobs=-1, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best estimator with tuned hyperparameters\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Fit the final model to the training data\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the final model on the training and validation data\n",
    "train_accuracy = best_model.score(X_train, y_train)\n",
    "val_accuracy = best_model.score(X_test, y_test)\n",
    "\n",
    "# Print the results\n",
    "print(\"Training Accuracy:\", train_accuracy)\n",
    "print(\"Validation Accuracy:\", val_accuracy)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1e96368c",
   "metadata": {},
   "source": [
    "Evaluate: Decision Tree Model\n",
    "Training Accuracy - 82.2%\n",
    "Validation Accuracy - 85.5%\n",
    "It has reduced overfitting compared to the initial model And Improve the result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee2d561",
   "metadata": {},
   "source": [
    "5.4 |KNeighborsClassifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468097fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a KNN classifier (you can adjust the number of neighbors 'n_neighbors')\n",
    "knn_model = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "# Fit the KNN model to the training data\n",
    "knn_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the training data\n",
    "y_train_pred = knn_model.predict(X_train)\n",
    "\n",
    "# Make predictions on the validation data\n",
    "y_val_pred = knn_model.predict(X_val)\n",
    "\n",
    "# Calculate the training and validation accuracies\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "\n",
    "print(\"Training Accuracy:\", train_accuracy)\n",
    "print(\"Validation Accuracy:\", val_accuracy)\n",
    "\n",
    "# Create a confusion matrix for validation data\n",
    "confusion = confusion_matrix(y_val, y_val_pred)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(confusion, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix (Validation)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6596bc18",
   "metadata": {},
   "source": [
    "Evaluate: KNeighborsClassifier: Before Tunning\n",
    "Training Accuracy - 80.0%\n",
    "Validation Accuracy - 66.00%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c491d805",
   "metadata": {},
   "source": [
    "Hyperparameter Tuning of KNN\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac03edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Define the hyperparameter grid to search\n",
    "param_grid = {\n",
    "    'n_neighbors': [1, 3, 5, 7, 9]  # Adjust the number of neighbors to explore\n",
    "}\n",
    "\n",
    "# Create the KNN classifier\n",
    "knn_model = KNeighborsClassifier()\n",
    "\n",
    "# Perform GridSearchCV for hyperparameter tuning\n",
    "grid_search = GridSearchCV(knn_model, param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best estimator with tuned hyperparameters\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions on the training data using the best model\n",
    "y_train_pred = best_model.predict(X_train)\n",
    "\n",
    "# Make predictions on the validation data using the best model\n",
    "y_val_pred = best_model.predict(X_val)\n",
    "\n",
    "# Calculate the training and validation accuracies\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "\n",
    "print(\"Training Accuracy with Best Hyperparameters:\", train_accuracy)\n",
    "print(\"Validation Accuracy with Best Hyperparameters:\", val_accuracy)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4991475c",
   "metadata": {},
   "source": [
    "Evaluate: KNN After the Tunning\n",
    "Training Accuracy - 79.4%\n",
    "Validation Accuracy - 72.7%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b141aa",
   "metadata": {},
   "source": [
    "5.5 |Support Vector Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1674ee6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "svm_model = SVC(kernel='linear')\n",
    "\n",
    "# Fit the SVM model to the training data\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the training data\n",
    "y_train_pred = svm_model.predict(X_train)\n",
    "\n",
    "# Make predictions on the validation data\n",
    "y_val_pred = svm_model.predict(X_val)\n",
    "\n",
    "# Calculate the training accuracy\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "\n",
    "# Calculate the validation accuracy\n",
    "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "\n",
    "print(\"Training Accuracy:\", train_accuracy)\n",
    "print(\"Validation Accuracy:\", val_accuracy)\n",
    "\n",
    "# Create confusion matrices\n",
    "train_confusion = confusion_matrix(y_train, y_train_pred)\n",
    "val_confusion = confusion_matrix(y_val, y_val_pred)\n",
    "\n",
    "# Plot the confusion matrices\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.heatmap(train_confusion, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix (Training)')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.heatmap(val_confusion, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix (Validation)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ae10f363",
   "metadata": {},
   "source": [
    "Evaluate: SVC\n",
    "Training Accuracy - 77.8%\n",
    "Validation Accuracy - 77.2%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e2f58a",
   "metadata": {},
   "source": [
    "5.6 |AdaBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4acb77d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "# Create an AdaBoost classifier\n",
    "adaboost_model = AdaBoostClassifier(n_estimators=50, random_state=42)\n",
    "\n",
    "# Fit the AdaBoost model to the training data\n",
    "adaboost_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the training data\n",
    "y_train_pred_adaboost = adaboost_model.predict(X_train)\n",
    "\n",
    "# Make predictions on the validation data\n",
    "y_val_pred_adaboost = adaboost_model.predict(X_val)\n",
    "\n",
    "# Calculate the training accuracy\n",
    "train_accuracy_adaboost = accuracy_score(y_train, y_train_pred_adaboost)\n",
    "\n",
    "# Calculate the validation accuracy\n",
    "val_accuracy_adaboost = accuracy_score(y_val, y_val_pred_adaboost)\n",
    "\n",
    "# Print the training and validation accuracies\n",
    "print(\"AdaBoost Training Accuracy:\", train_accuracy_adaboost)\n",
    "print(\"AdaBoost Validation Accuracy:\", val_accuracy_adaboost)\n",
    "\n",
    "# Create a confusion matrix for validation\n",
    "confusion_adaboost = confusion_matrix(y_val, y_val_pred_adaboost)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure()\n",
    "sns.heatmap(confusion_adaboost, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('AdaBoost Confusion Matrix (Validation)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "27f67c7c",
   "metadata": {},
   "source": [
    "Evaluate: AdaBoost Classifier¶\n",
    "Training Accuracy - 82.4%\n",
    "Validation Accuracy - 74.0%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acea9fec",
   "metadata": {},
   "source": [
    "5.7 | Gradient Boosting Classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1510b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Create a Gradient Boosting classifier\n",
    "gbm_model = GradientBoostingClassifier(n_estimators=100, max_depth=3, random_state=42)\n",
    "\n",
    "# Fit the GBM model to the training data\n",
    "gbm_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the training data\n",
    "y_train_pred_gbm = gbm_model.predict(X_train)\n",
    "\n",
    "# Make predictions on the validation data\n",
    "y_val_pred_gbm = gbm_model.predict(X_val)\n",
    "\n",
    "# Calculate the training accuracy\n",
    "train_accuracy_gbm = accuracy_score(y_train, y_train_pred_gbm)\n",
    "\n",
    "# Calculate the validation accuracy\n",
    "val_accuracy_gbm = accuracy_score(y_val, y_val_pred_gbm)\n",
    "\n",
    "# Print the training and validation accuracies\n",
    "print(\"GBM Training Accuracy:\", train_accuracy_gbm)\n",
    "print(\"GBM Validation Accuracy:\", val_accuracy_gbm)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "566e1608",
   "metadata": {},
   "source": [
    "Evaluate: Gradient Boosting Classifier\n",
    "Training Accuracy - 94.2%\n",
    "Validation Accuracy - 75.3%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849c698c",
   "metadata": {},
   "source": [
    "5.8 | xgboost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342283ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Create an XGBoost classifier\n",
    "xgboost_model = XGBClassifier(n_estimators=100, max_depth=3, random_state=42)\n",
    "\n",
    "# Fit the XGBoost model to the training data\n",
    "xgboost_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the training data\n",
    "y_train_pred_xgboost = xgboost_model.predict(X_train)\n",
    "\n",
    "# Make predictions on the validation data\n",
    "y_val_pred_xgboost = xgboost_model.predict(X_val)\n",
    "\n",
    "# Calculate the training accuracy\n",
    "train_accuracy_xgboost = accuracy_score(y_train, y_train_pred_xgboost)\n",
    "\n",
    "# Calculate the validation accuracy\n",
    "val_accuracy_xgboost = accuracy_score(y_val, y_val_pred_xgboost)\n",
    "\n",
    "# Print the training and validation accuracies\n",
    "print(\"XGBoost Training Accuracy:\", train_accuracy_xgboost)\n",
    "print(\"XGBoost Validation Accuracy:\", val_accuracy_xgboost)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "217a6511",
   "metadata": {},
   "source": [
    "Evaluate: xgboost Classifier\n",
    "Training Accuracy - 98.4%\n",
    "Validation Accuracy - 72.7%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634a6d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the scores and models\n",
    "# (Assuming you have already calculated the test scores for each algorithm)\n",
    "\n",
    "# Replace the test scores below with the actual test scores you obtained from the code\n",
    "\n",
    "Logisticregressionscoretest = 0.81\n",
    "KNeighborsClassifierModelscoretest = 0.88\n",
    "GaussianNBModelscoretest = 0.76\n",
    "SVCModelscoretest = 0.87\n",
    "DecisionTreeClassifierModeltest = 0.93\n",
    "RandomForestClassifierModeltest = 0.89\n",
    "gb_clftest = 0.86\n",
    "xgb_test = 0.91\n",
    "\n",
    "# Create the DataFrame\n",
    "models = pd.DataFrame({\n",
    "    'Model': ['Logistic Regression', 'KNN', 'Naive Bayes', 'Linear SVC', 'Decision Tree', 'Random Forest', 'Gradient Booster', 'XGBoost'],\n",
    "    'Test Score': [Logisticregressionscoretest, KNeighborsClassifierModelscoretest, GaussianNBModelscoretest, SVCModelscoretest, DecisionTreeClassifierModeltest, RandomForestClassifierModeltest, gb_clftest, xgb_test]\n",
    "})\n",
    "\n",
    "# Plot the test scores\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(models['Model'], models['Test Score'], color='b', alpha=0.7)\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Test Score')\n",
    "plt.title('Test Scores for Different Algorithms')\n",
    "plt.ylim(0.7, 1.0)  # Set the y-axis limits for better visualization\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea43bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install termcolor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5f17e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tabulate import tabulate\n",
    "from termcolor import colored\n",
    "\n",
    "# Define the scores and models\n",
    "# ... (your score definitions here)\n",
    "\n",
    "# Create the DataFrame\n",
    "models = pd.DataFrame({\n",
    "    'Model': ['Logistic Regression',\n",
    "              'KNN',\n",
    "              'Naive Bayes',\n",
    "              'Linear SVC',\n",
    "              'Decision Tree',\n",
    "              'Random Forest',\n",
    "              'Gradient Booster',\n",
    "              'XGBoost'\n",
    "             ],\n",
    "    'Train Score': [Logisticregressionscoretraining,\n",
    "                    KNeighborsClassifierModelscoretraining,\n",
    "                    GaussianNBModelscoretrain,\n",
    "                    SVCModelscoretraining,\n",
    "                    DecisionTreeClassifierModeltrain,\n",
    "                    RandomForestClassifierModeltrain,\n",
    "                    gb_clftrain,\n",
    "                    xgb_train\n",
    "                   ],\n",
    "    'Test Score': [Logisticregressionscoretest,\n",
    "                   KNeighborsClassifierModelscoretest,\n",
    "                   GaussianNBModelscoretest,\n",
    "                   SVCModelscoretest,\n",
    "                   DecisionTreeClassifierModeltest,\n",
    "                   RandomForestClassifierModeltest,\n",
    "                   gb_clftest,\n",
    "                   xgb_test\n",
    "                  ]\n",
    "})\n",
    "\n",
    "# List of colors to cycle through\n",
    "colors = ['red', 'green', 'blue', 'magenta', 'yellow', 'cyan', 'white']\n",
    "\n",
    "# Prepare the formatted table rows\n",
    "formatted_rows = []\n",
    "for idx, row in models.iterrows():\n",
    "    model_name = row['Model']\n",
    "    train_score = row['Train Score']\n",
    "    test_score = row['Test Score']\n",
    "    \n",
    "    color_idx = idx % len(colors)\n",
    "    formatted_model = colored(model_name, colors[color_idx])\n",
    "    formatted_row = [formatted_model, f'{train_score:.2f}', f'{test_score:.2f}']\n",
    "    formatted_rows.append(formatted_row)\n",
    "\n",
    "# Print the formatted table\n",
    "headers = ['Model', 'Train Score', 'Test Score']\n",
    "table = tabulate(formatted_rows, headers=headers, tablefmt='grid')\n",
    "print(table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1a5405",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
